{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccddce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_model.ipynb\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# --- Load model and features ---\n",
    "model = joblib.load(\"../app/model.pkl\")\n",
    "model_features = joblib.load(\"../app/model_features.pkl\")\n",
    "\n",
    "print(\"âœ… Model loaded successfully\")\n",
    "print(\"Model type:\", type(model))\n",
    "\n",
    "# Identify model type\n",
    "if hasattr(model, \"coef_\"):\n",
    "    print(\"This looks like a Linear Model (e.g., Logistic Regression).\")\n",
    "elif hasattr(model, \"feature_importances_\"):\n",
    "    print(\"This looks like a Tree-based Model (e.g., Random Forest, Gradient Boosting).\")\n",
    "else:\n",
    "    print(\"Model type could not be determined directly.\")\n",
    "\n",
    "# --- Load dataset ---\n",
    "df = pd.read_csv(\"../data/churn_clean.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = pd.get_dummies(df.drop(columns=[\"Churn\"]))\n",
    "X = X.reindex(columns=model_features, fill_value=0)\n",
    "y = df[\"Churn\"]\n",
    "\n",
    "# --- Predictions ---\n",
    "y_pred = model.predict(X)\n",
    "y_proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "print(\"\\nðŸ“Š Model Evaluation Results:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y, y_pred):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y, y_pred):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y, y_proba):.4f}\")\n",
    "\n",
    "# --- Detailed Classification Report ---\n",
    "print(\"\\nðŸ“„ Classification Report:\")\n",
    "print(classification_report(y, y_pred))\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print(\"ðŸ§¾ Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# =======================\n",
    "# ðŸ“Š Visualization Section\n",
    "# =======================\n",
    "\n",
    "# --- Confusion Matrix Heatmap ---\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Churn\", \"Churn\"], yticklabels=[\"No Churn\", \"Churn\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# --- ROC Curve ---\n",
    "fpr, tpr, thresholds = roc_curve(y, y_proba)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_score(y, y_proba):.4f}\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
